{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "import ipdb\n",
    "import time\n",
    "from utils import ZCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_side_length = 11\n",
    "stride = 1\n",
    "multiscale = False # multilayer pipeline to preprocess the images\n",
    "uniform_noise = True # adds uniform noise in the range [0,1] on each pixel\n",
    "full_image = False # full images instead of patches are used\n",
    "save_as_double = True # choose to save as float or double\n",
    "batch_size = 1000\n",
    "shuffle = True\n",
    "whitening = False\n",
    "plot_covariance = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(),])\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "train_folder = 'cifar_train/'\n",
    "test_folder = 'cifar_test/'\n",
    "# classes = ('plane', 'car', 'bird', 'ct', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "classes = ('plane', 'car')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchWriter(torch.nn.Module):\n",
    "    def __init__(self, crop_side_length, stride, multiscale):\n",
    "        super(PatchWriter, self).__init__()\n",
    "        self.pool = torch.nn.AvgPool2d(crop_side_length, stride=crop_side_length//2)\n",
    "        self.crop_side_length = crop_side_length\n",
    "        self.stride = stride\n",
    "        self.multiscale = multiscale\n",
    "\n",
    "    # define pipeline here\n",
    "    def forward(self, images):\n",
    "        all_patches = []\n",
    "        all_cells = []\n",
    "        for image in images:\n",
    "            patch_list = []\n",
    "            cell_list = []\n",
    "            patches, cells = self.extract_patches(image) # original scale\n",
    "            patch_list.append(patches) # original scale\n",
    "            cell_list.append(cells) # original scale\n",
    "            if self.multiscale:\n",
    "                image = self.pool(image)\n",
    "                patches, cells = self.extract_patches(image) # pool 1\n",
    "                patch_list.append(patches) # pool 1\n",
    "                cell_list.append(cells) # pool 1\n",
    "                image = self.pool(image)\n",
    "                patches, cells = self.extract_patches(image) # pool 2\n",
    "                patch_list.append(patches) # pool 2\n",
    "                cell_list.append(cells) # pool 2\n",
    "            patches = np.vstack(patch_list)\n",
    "            cells = np.vstack(cell_list)\n",
    "            all_patches.append(patches)\n",
    "            all_cells.append(cells)\n",
    "        return np.concatenate(all_patches).reshape(images.shape[0], *patches.shape), np.concatenate(all_cells).reshape(images.shape[0], *cells.shape)\n",
    "\n",
    "    # given an image, crop so many times and return array of patches\n",
    "    def extract_patches(self, image):\n",
    "        n_patches = image.shape[-1]//stride+1\n",
    "        image = np.squeeze(image)\n",
    "        patches = np.zeros((n_patches**2, image.shape[0], self.crop_side_length**2), dtype=np.float64)  \n",
    "        cells = np.zeros((n_patches**2))\n",
    "        image = transforms.functional.to_pil_image(image)\n",
    "        image = transforms.functional.pad(image, self.crop_side_length//2)\n",
    "        for i in range(n_patches):\n",
    "            for j in range(n_patches):\n",
    "                image_cropped = transforms.functional.crop(image, i*stride, j*stride, self.crop_side_length, self.crop_side_length)\n",
    "                image_tensors = transforms.functional.to_tensor(image_cropped)\n",
    "                patches[i*n_patches+j][:] = image_tensors.reshape(-1, self.crop_side_length**2)\n",
    "                if i<n_patches/2 and j<n_patches/2:\n",
    "                    cells[i*n_patches+j]=0\n",
    "                elif i<n_patches/2 and j>n_patches/2:\n",
    "                    cells[i*n_patches+j]=1\n",
    "                elif i>n_patches/2 and j<n_patches/2:\n",
    "                    cells[i*n_patches+j]=2\n",
    "                elif i>n_patches/2 and j>n_patches/2:\n",
    "                    cells[i*n_patches+j]=3\n",
    "        return patches,cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_patches_to_disk(dataset, folder, crop_side_length, stride, multiscale, uniform_noise, full_image, save_as_double):\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=4)\n",
    "    if full_image:\n",
    "        for f in [folder, folder+'data/', folder+'labels/']:\n",
    "            if not os.path.exists(f): os.makedirs(f)\n",
    "            for fname in os.listdir(f):\n",
    "                if not os.path.isdir(f+fname): os.remove(f+fname)\n",
    "    else:\n",
    "        for f in [folder, folder+'data/', folder+'labels/', folder+'cells/']:\n",
    "            if not os.path.exists(f): os.makedirs(f)\n",
    "            for fname in os.listdir(f):\n",
    "                if not os.path.isdir(f+fname): os.remove(f+fname)\n",
    "    \n",
    "    data_type = np.float64 if save_as_double else np.float32\n",
    "    model = PatchWriter(crop_side_length, stride, multiscale)\n",
    "\n",
    "    for index, images_and_labels in enumerate(tqdm(iter(dataloader))):\n",
    "        images, labels = images_and_labels\n",
    "        data_file_name = './' + folder + 'data/' + str(index) + '.npy'\n",
    "        label_file_name = './' + folder + 'labels/' + str(index) + '.npy'\n",
    "        np.save(label_file_name, labels.numpy().astype(np.int32))\n",
    "        if uniform_noise: images = images + torch.Tensor(np.random.uniform(0,1, images.shape))\n",
    "        if full_image:\n",
    "            images_np = images.numpy()\n",
    "            images_np = images_np.reshape(images.shape[0], 1, 1, -1)\n",
    "            np.save(data_file_name, images_np.astype(data_type))\n",
    "        else:\n",
    "            cell_file_name = './' + folder + 'cells/' + str(index) + '.npy'\n",
    "            patches,cells = model(images)\n",
    "            np.save(data_file_name, patches.astype(data_type))  \n",
    "            np.save(cell_file_name, cells.astype(data_type))            \n",
    "    \n",
    "    C = 1\n",
    "    D = 3*(images.shape[-1]**2) if full_image else crop_side_length**2\n",
    "    Total = len(dataloader)*batch_size\n",
    "    if not full_image: Total *= patches.shape[1]\n",
    "    np.savetxt('./' + folder +'header.txt', np.array([Total, batch_size, C, D]), delimiter=',', fmt='%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save the patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write_patches_to_disk(trainset, train_folder, crop_side_length, stride, multiscale, uniform_noise, full_image, save_as_double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_patches_to_disk(testset, test_folder, crop_side_length, stride, multiscale, uniform_noise, full_image, save_as_double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if whitening:\n",
    "    train_data = [] \n",
    "    for i,f in enumerate(os.listdir(train_folder+\"data/\")):\n",
    "        if i%3==0: train_data.append(np.load(train_folder+\"data/\"+f))\n",
    "    print(len(train_data))\n",
    "    train_data = np.concatenate(train_data)\n",
    "    \n",
    "    orig_shape = train_data.shape\n",
    "    train_data = train_data.reshape((train_data.shape[0],-1))\n",
    "\n",
    "    zca = ZCA(bias=1e-8)\n",
    "    zca.fit(train_data)\n",
    "    train_data = zca.transform(train_data)\n",
    "    train_data = train_data.reshape(orig_shape)\n",
    "    \n",
    "    for i, step in enumerate(range(0,train_data.shape[0],batch_size)):\n",
    "        print(i)\n",
    "        np.save(train_folder+\"data/{}.npy\".format(i),train_data[step:step+batch_size])\n",
    "        \n",
    "        \n",
    "    \n",
    "    test_data = [] \n",
    "    for f in os.listdir(test_folder+\"data/\"):\n",
    "        test_data.append(np.load(test_folder+\"data/\"+f))\n",
    "    test_data = np.concatenate(test_data)\n",
    "    print(len(test_data))\n",
    "    \n",
    "    orig_shape = test_data.shape\n",
    "    test_data = test_data.reshape((test_data.shape[0],-1))\n",
    "\n",
    "    test_data = zca.transform(test_data)\n",
    "    test_data = test_data.reshape(orig_shape)\n",
    "    \n",
    "    for i, step in enumerate(range(0,test_data.shape[0],batch_size)):\n",
    "        print(i)\n",
    "        np.save(test_folder+\"data/{}.npy\".format(i),test_data[step:step+batch_size])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### covariances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot_covariance:\n",
    "    import seaborn as sn\n",
    "    import matplotlib.pyplot as plt\n",
    "    dataset_patches = np.concatenate([np.load(test_folder+\"data/\"+f)[0] for f in os.listdir(test_folder+\"data/\") if f.endswith(\".npy\")])\n",
    "    dataset_patches = np.array([0.2989 * p[0,:] + 0.5870 *  p[1,:] + 0.1140 * p[2,:] for p in dataset_patches])\n",
    "    covariance = np.corrcoef(dataset_patches.T)\n",
    "    sn.heatmap(covariance)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "nteract": {
   "version": "0.22.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
