{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import tonic\n",
    "import tonic.transforms as transforms\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "from utils import write_dataset, custom_sampler\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'POKERDVS' # name of dataset: POKERDVS -- NMNIST -- NCARS -- DVSGesture\n",
    "surface_dimensions = [9,9] # surface dimensions not necessarily square\n",
    "time_constant = 80e3 # decay time constant for time surfaces\n",
    "download_dataset = True # downloads the datasets before parsing\n",
    "first_saccade_only = True # specific for N-MNIST (3 saccades 100ms each)\n",
    "denoise = False # denoising that removes solitary events\n",
    "denoise_filter = 1e3 # decrease time filter to remove more events if denoising is turned on\n",
    "split_grid = True # save location of timesurfaces in grid. useful to build spatial histograms\n",
    "K = 10 # cell size (only works with split_grid)\n",
    "decay='exp'\n",
    "add_noise = False\n",
    "refractory = True # add refractory period\n",
    "refractory_period = 0.5\n",
    "subsample = 100 # take a sample of the dataset\n",
    "plot_dictionary = False\n",
    "plot_covariance = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTimesurface(surface_dimensions=surface_dimensions, tau=time_constant, decay=decay, merge_polarities=True)])\n",
    "    \n",
    "if denoise: transform.transforms = [transforms.Denoise(time_filter=denoise_filter)] + transform.transforms \n",
    "if refractory: transform.transforms = [transforms.RefractoryPeriod(refractory_period=refractory_period)] + transform.transforms \n",
    "    \n",
    "if dataset_name == 'NCARS': # 304 x 240\n",
    "    train_set = tonic.datasets.NCARS(save_to='./data', train=True, download=download_dataset)\n",
    "    test_set = tonic.datasets.NCARS(save_to='./data', train=False, download=download_dataset)\n",
    "if dataset_name == 'POKERDVS': # 35 x 35\n",
    "    train_set = tonic.datasets.POKERDVS(save_to='./data', train=True, download=download_dataset)\n",
    "    test_set = tonic.datasets.POKERDVS(save_to='./data', train=False, download=download_dataset)\n",
    "elif dataset_name == \"DVSGesture\": # 128 x 128\n",
    "    train_set = tonic.datasets.DVSGesture(save_to='./data', train=True, download=download_dataset)\n",
    "    test_set = tonic.datasets.DVSGesture(save_to='./data', train=False, download=download_dataset)\n",
    "elif dataset_name == 'NMNIST': # 34 x 34\n",
    "    train_set = tonic.datasets.NMNIST(save_to='./data/nmnist', train=True, download=download_dataset, first_saccade_only=first_saccade_only)\n",
    "    test_set = tonic.datasets.NMNIST(save_to='./data/nmnist', train=False, download=download_dataset, first_saccade_only=first_saccade_only)\n",
    "\n",
    "train_folder = decay + '/' + dataset_name.lower()\n",
    "train_folder = train_folder + \"_train/\" if not split_grid else train_folder + \"_train_sp%s/\" % K\n",
    "\n",
    "test_folder = decay + '/' + dataset_name.lower()\n",
    "test_folder = test_folder + \"_test/\" if not split_grid else test_folder + \"_test_sp%s/\" % K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating indices (to shuffle dataset and/or create a subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index = np.arange(len(train_set))\n",
    "np.random.shuffle(train_index)\n",
    "\n",
    "test_index = np.arange(len(test_set))\n",
    "np.random.shuffle(test_index)\n",
    "\n",
    "if subsample > 0 and subsample < 100:\n",
    "    print(\"Taking %s%% of the dataset\" % subsample)\n",
    "    \n",
    "    # calculate number of samples we want to take\n",
    "    train_samples = np.ceil((subsample * len(train_set)) / 100).astype(int)\n",
    "    test_samples = np.ceil((subsample * len(test_set)) / 100).astype(int)\n",
    "    \n",
    "    # choosing indices of the subset\n",
    "    train_index = train_index[:train_samples]\n",
    "    test_index = test_index[:test_samples]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_surf_shapes, train_cell_index = write_dataset(train_set, train_index, train_folder, transform, surface_dimensions, True, add_noise, split_grid, K)\n",
    "test_surf_shapes, test_cell_index = write_dataset(test_set, test_index, test_folder, transform, surface_dimensions, True, add_noise, split_grid, K)\n",
    "\n",
    "tr_Total = sum([shape[1] for shape in train_surf_shapes]) # number of training data points\n",
    "te_Total = sum([shape[1] for shape in test_surf_shapes]) # number of test data points\n",
    "D = test_surf_shapes[0][-1] # dimensions\n",
    "C = test_surf_shapes[0][2] # number of channels\n",
    "\n",
    "np.savetxt(train_folder+'/header.txt', np.array([tr_Total, 1, C, D, train_cell_index]), delimiter=',', fmt='%d')\n",
    "np.savetxt(test_folder+'/header.txt', np.array([te_Total, 1, C, D, test_cell_index]), delimiter=',', fmt='%d')\n",
    "\n",
    "# save dataset shuffle indices\n",
    "np.savetxt(train_folder+'/indices.txt', train_index, fmt='%i')\n",
    "np.savetxt(test_folder+'/indices.txt', test_index, fmt='%i')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionary of timesurfaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot_dictionary:\n",
    "    import matplotlib.pyplot as plt\n",
    "    timesurfaces = np.load(test_folder+'data/5.npy')[0]\n",
    "    timesurfaces = timesurfaces.reshape((timesurfaces.shape[0],timesurfaces.shape[1],int(np.sqrt(timesurfaces.shape[2])),int(np.sqrt(timesurfaces.shape[2]))))\n",
    "    rows = 10\n",
    "    cols = 10\n",
    "    fig, axes = plt.subplots(rows,cols, figsize=(8,8))\n",
    "    for i,ax in enumerate(axes.flat):\n",
    "        ax.imshow(timesurfaces[i+2500,0], cmap='hot')\n",
    "        ax.axis('off')\n",
    "    fig.savefig(test_folder+'/dic_timesurfaces.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### covariances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot_covariance:\n",
    "    import seaborn as sn\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    dataset_ts = np.squeeze(np.concatenate([np.load(test_folder+\"data/\"+f)[0] for f in os.listdir(test_folder+\"data/\") if f.endswith(\".npy\")]))\n",
    "    dataset_ts = np.delete(dataset_ts, int(np.floor(dataset_ts[0].shape[0] / 2)), axis=1)\n",
    "    covariance = np.corrcoef(dataset_ts.T)\n",
    "    sn.heatmap(covariance)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
