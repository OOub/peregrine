{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this notebook we perform a clustering analysis from variational GMMs with a 2D artificial dataset (N=5000 D=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import subprocess\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from utils import read_parameters, plot_center_evolution\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from time import gmtime, strftime\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# data parameters\n",
    "name                    = 'artificial'\n",
    "training_path           = '../data-generation/'+name+'/artificial.npy' # path to training features\n",
    "test_path               = '' # path to test features (empty string if we don't want a test set)\n",
    "sample_percentage       = 100 # take a data sample for the training set instead of full data (between 1 and 100)\n",
    "\n",
    "# model parameters\n",
    "algorithm               = 2 # 0:k-means | 1:u-S-GMM | 2:S-GMM\n",
    "M                       = 20 # number of cluster centers\n",
    "H                       = 3 # number of clusters considered for each data point\n",
    "R                       = 6 # number of new samples\n",
    "Nprime                  = 0 # size of subset | set to 0 to disable coreset creation\n",
    "stream                  = False # stream data from harddrive to build coreset | training data not loaded and only works when using coresets\n",
    "chain_length            = 10 # chain length for AFK-MCÂ² seeding\n",
    "convergence_threshold   = 0.0001 # < 1 for convergence threshold | >= 1 for epochs\n",
    "\n",
    "# output parameters\n",
    "trials                  = 100 # number of runs in parallel\n",
    "top_k                   = 1 # save the k best clusters during inference\n",
    "inference               = False # save cluster assignments for training and test datasets (when streaming doesn't work for training set)\n",
    "save_additional_info    = 1 # 1: save centers, error, and priors across iterations | 2: only priors\n",
    "verbose                 = 1 # display output in C++ (1 or 2 for even more information)\n",
    "save_figures            = True # save figures after plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run C++ GMM standalone executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "save_path = Path('results/'+name+str(M))\n",
    " \n",
    "if algorithm == 0:\n",
    "    # k-means baselines\n",
    "    cmd_list = ['../build/release/km', \n",
    "                training_path, \n",
    "                test_path, \n",
    "                str(save_path), \n",
    "                str(sample_percentage),\n",
    "                str(M), \n",
    "                str(Nprime),\n",
    "                str(int(stream)), \n",
    "                str(convergence_threshold), \n",
    "                str(trials), \n",
    "                str(int(inference)),\n",
    "                str(int(save_additional_info)), \n",
    "                str(int(verbose)), \n",
    "               ]\n",
    "else:\n",
    "    # gmm algorithms\n",
    "    cmd_list = ['../build/release/gmm',\n",
    "                training_path, \n",
    "                test_path, \n",
    "                str(save_path),\n",
    "                str(sample_percentage),\n",
    "                str(algorithm),\n",
    "                str(M),   \n",
    "                str(H),                                   \n",
    "                str(R),                                    \n",
    "                str(Nprime),\n",
    "                str(int(stream)),                        \n",
    "                str(chain_length),                                     \n",
    "                str(convergence_threshold), \n",
    "                str(trials),\n",
    "                str(top_k),\n",
    "                str(int(inference)),\n",
    "                str(0),\n",
    "                str(int(save_additional_info)), \n",
    "                str(int(verbose)),\n",
    "               ]\n",
    "\n",
    "# running C++ executable within Python\n",
    "p = subprocess.run(cmd_list, capture_output=True, universal_newlines=True)\n",
    "if len(p.stderr) > 0: print(p.stderr)\n",
    "    \n",
    "print('--- %s seconds ---' % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_parameters = read_parameters(save_path/'0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Centers across iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import rc\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "# creating animation\n",
    "anim = plot_center_evolution(save_path/'0', ground_truth='../data-generation/'+name+'/artificial_centroids.npy', save_figure=save_figures)\n",
    "anim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization error across iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = [np.load(save_path/'0'/'tr_quantization'/str(str(i)+'.npy'))[0] for i in model_parameters.iteration]\n",
    "\n",
    "fig2 = plt.figure()\n",
    "plt.plot(model_parameters.iteration[1:], error[1:])\n",
    "plt.title('Quantization error across iterations')\n",
    "plt.show()\n",
    "\n",
    "if save_figures:\n",
    "    fig2.savefig(save_path/'quantization_iters.pdf', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Free energy across iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig3 = plt.figure()\n",
    "plt.plot(model_parameters.iteration[1:], model_parameters.free_energy[1:])\n",
    "plt.title('Free energy across iterations')\n",
    "plt.show()\n",
    "\n",
    "if save_figures:\n",
    "    fig3.savefig(save_path/'free_energy_iters.pdf', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigma across iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig4 = plt.figure()\n",
    "plt.plot(model_parameters.iteration[1:], model_parameters.sigma[1:])\n",
    "plt.title('Sigma across iterations')\n",
    "plt.show()\n",
    "\n",
    "if save_figures:\n",
    "    fig4.savefig(save_path/'sigma_iters.pdf', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average prior decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if algorithm == 2 and save_additional_info > 0: # only works with a fixed number of iterations\n",
    "    priors = np.zeros((trials, M))\n",
    "    for i in np.arange(trials):\n",
    "        model_parameters = read_parameters(save_path/str(i))\n",
    "        prior_path = save_path/str(i)/'priors'\n",
    "        priors[i] = np.sort(np.array([np.load(prior_path/str(str(j)+'.npy')) for j in model_parameters.iteration]).T[:,-1])[::-1]\n",
    "     \n",
    "    initial_prior = np.load(save_path/'0'/'priors'/'0.npy')\n",
    "    mean_priors = np.mean(priors, axis=0)\n",
    "    std_priors = np.std(priors, axis=0)\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    plt.hlines(y=initial_prior, linestyle=':', xmin=0, xmax=M-1, label='Prior distribution before training')\n",
    "    plt.errorbar(np.arange(len(mean_priors)),mean_priors, std_priors,label='Prior distribution after training')\n",
    "\n",
    "    plt.xlabel('Cluster number')\n",
    "    ax.set_xticks(range(0,len(mean_priors),2))\n",
    "    ax.set_xticklabels(np.arange(1,len(mean_priors)+1,2))\n",
    "    plt.legend()\n",
    "    ax.set_ylabel(r'prior ($\\alpha$) probability')\n",
    "\n",
    "    if save_figures:\n",
    "        fig.savefig(save_path/str('priors_decay%s.pdf' %M))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('--- %s seconds ---' % (time.time() - start_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "nteract": {
   "version": "0.22.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
